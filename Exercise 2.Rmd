---
title: "Exercise 2"
author: "JAST"
date: "August 17, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Question 1

Think about zoning down in the period: May 2008 and December 2008 with summer winter break
For this period vs. the other, focus if 

For Austin:
1. Which is the busiest day?
2. Where is everyone coming from?
3. Which flight carrier should one avoid?

* Operational Efficiency - TaxiIn TaxiOut
* Cancels the most
* Security Delay - Arrival and Departure

```{r}
###############Q1########################

library(tidyverse)
library(ggplot2)
library(dplyr)
library(ggmap)

set.seed(145)
df = read.csv("C:/Users/teeru/Downloads/ABIA.csv")

```

```{r}
df = df %>%
  mutate_at(c(25:29), funs(replace(.,is.na(.),0))) %>%
  mutate_at(15, funs(replace(.,is.na(.),0))) %>%
  mutate ("Flight" = paste(UniqueCarrier,FlightNum) ) %>%
  mutate("Late Arrival" = ifelse(ArrDelay > 0 , 1, 0)) %>%
  mutate("Late Carrier" = ifelse(CarrierDelay > 0,1, 0)) %>%
  mutate("Late Aircraft" = ifelse(LateAircraftDelay > 0, 1, 0)) %>%
  mutate("OpEff" = `Late Arrival` + `Late Carrier` + `Late Aircraft` + Cancelled)

```

To check the number of flights per month
```{r}
ggplot(df, aes(x= factor(Month))) +geom_bar(stat = "count") + labs(x = "Month", y= "Number of flights", title = "Number of flights in the year 2008") +coord_flip()

```

  Since May to August is a busy time especially considering graduation, vacations and new college year start we will look at this data in more detail
  
  
```{r include=FALSE}
df = df%>%
  filter(Month == 5 | Month == 6| Month ==7 | Month ==8)
```

Checking which states have the maximum arrival or departure delay

```{r}
####Create a map for the 
air = read.csv("C:/Users/teeru/Downloads/183806017_T_MASTER_CORD.csv")

dup = air[,1]
air_ll = air[!duplicated(dup),]
air_ll = air_ll[,c(1,4,6,8)]

df = merge(x = df, y = air_ll, all.x = TRUE, by.x = "Dest", by.y = "AIRPORT")
df = merge(x = df, y = air_ll, all.x = TRUE, by.x = "Origin", by.y = "AIRPORT", suffix =c(".dest",".org"))

df_origin = df %>%
  filter(Origin == "AUS") 

df_dest = df %>%
  filter(Dest == "AUS")

```
Every flight is domestic. Hence considering the map of US

```{r}
origin = df_origin %>%
  mutate_at(15, funs(replace(.,is.na(.),0))) %>%
  group_by(AIRPORT_STATE_NAME.dest) %>%
  summarize(flights = mean(DepDelay))

destination = df_dest %>%
  group_by(AIRPORT_STATE_NAME.org) %>%
  summarize(flights = mean(ArrDelay))

```

Since departure delays are very few, we will only look at arrival delays
```{r}
library(maps)
us_states= map_data("state")
destination$AIRPORT_STATE_NAME.org = tolower(destination$AIRPORT_STATE_NAME.org)
us_states_flight_dest = merge(x = us_states, y = destination, all.x = TRUE, by.x="region", by.y = "AIRPORT_STATE_NAME.org")
us_states_flight_dest = us_states_flight_dest %>%
  mutate_at(7, funs(replace(.,is.na(.),0)))

```

```{r}
library(ggplot2)
library(fiftystater)

data("fifty_states") # this line is optional due to lazy data loading
q <- ggplot(us_states_flight_dest, aes(map_id = region)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = flights), map = fifty_states) + 
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank()) +
  labs(title ="Flights departing from Austin to other cities")

q

```

As seen above the flights to the state of Virginia have the maximum departure delay.

To see which carriers to avoid, we calculated the operational efficieny of the carriers. The operational efficiency was calculated as the weighted average of the following variables:
1. Cancelled Flights
2. Delayed Arrivals
3. Late Aircrafts
4. Delays due to the carrier

While there may be some overlap in the above variables, but we have assumed none.

```{r}
ops = df %>%
  group_by(UniqueCarrier, OpEff) %>%
  summarise( cn = n()) %>%
  mutate(performance = weighted.mean(cn,OpEff))

cls = c(1,4)
perfid = unique(ops[,cls])

ggplot(perfid, aes(x = UniqueCarrier , y = performance)) + geom_bar(stat = "identity", fill = "#0000CC") + labs(title = "Operational Delays of Carriers in 2008", x = "Carriers", y="Operational Delays")

```


In the above graph the lower the height of the bar, the better the operational performance of the carrier.


```{r}
#busiset day
arr = df %>%
  group_by(DayOfWeek) %>%
  summarise(arr_delay = mean(ArrDelay))
  
dep = df %>%
  mutate_at(18, funs(replace(.,is.na(.),0))) %>%
  group_by(DayOfWeek) %>%
  summarise(dep_delay = mean(DepDelay))

par(mfrow=c(1,2))
ggplot(arr,aes(x=DayOfWeek, y = arr_delay)) + geom_line() + geom_point()+ labs(title = "Arrival delays of flights per day in May - Aug 2008", x = "Day of week",y="Mean of arrival delays") 
ggplot(dep,aes(x=DayOfWeek, y = dep_delay)) + geom_line() + geom_point()+ labs(title = "Departure delays of flights per day in May - Aug 2008", x = "Day of week",y="Mean of departure delays") 

```

Wednesday is the day with least arrival and departure delays



Results:
If you want to travel and be on time in the months of May to Aug, travel by NW airline carrier on a Wednesday.Also the flights to Virginia, New York and North Carolina had the most departure delay. Hence, for a good start to your vacation, you might want to avoid these states.

# Question 2

Predict the author of the article based on article's textual context

## Step 1: Reading in the files from C50train and C50test directories

```{r}
library(tm)

#Wrapper function
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

author_dirs_train = Sys.glob('ReutersC50/C50train/*')
author_dirs_test = Sys.glob('ReutersC50/C50test/*')

```

We have imported all the authors files for training into `author_dirs_train`. Now we will clean these files.

```{r}
# Rolling all directories together into a single corpus and getting Author names
file_list_train = NULL
labels = NULL
for(author in author_dirs_train) 
{
  author_name = substring(author, first = 29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

#Getting rid of '.txt' from filename
all_docs_train = lapply(file_list_train, readerPlain) 
names(all_docs_train) = file_list_train
names(all_docs_train) = sub('.txt', '', names(all_docs_train))


file_list_test = NULL
labels = NULL
for(author in author_dirs_test) 
{
  author_name = substring(author, first = 29)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

#Getting rid of '.txt' from filename
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))
```

`all_docs_train` has all the files. `author_name` has the name of the authors who wrote the corresponding files.
`file_list_train` has the full path of all the files.

## Step 2: Creating the Document Term Matrix and TF-IDF for training

```{r}

# Creating training corpus and processing
my_corpus_train = Corpus(VectorSource(all_docs_train))
#names(my_corpus_train) = sapply(strsplit(names(all_docs_train), "/"), "[", 3)

# Preprocessing
my_corpus_train = tm_map(my_corpus_train, content_transformer(tolower)) # make everything lowercase
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeNumbers)) # remove numbers
my_corpus_train = tm_map(my_corpus_train, content_transformer(removePunctuation)) # remove punctuation
my_corpus_train = tm_map(my_corpus_train, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("SMART"))
my_corpus_train = tm_map(my_corpus_train, content_transformer(removeWords), stopwords("en"))

DTM_train= DocumentTermMatrix(my_corpus_train)

DTM_train# some basic summary statistics

DTM_train = removeSparseTerms(DTM_train, 0.95)
DTM_train # now ~ 660 terms (versus ~32000 before)
inspect(DTM_train[1:10,1:10])
# construct TF IDF weights
tfidf_train = weightTfIdf(DTM_train)

```

## Step 3: Creating the Document Term Matrix and TF-IDF for testing

NOTE: The words which aren't present in the training data are dropped here from the test DTM.

```{r}
my_corpus_test = Corpus(VectorSource(all_docs_test))
#names(my_corpus_test) = sapply(strsplit(names(all_docs_test), "/"), "[", 3)

# Preprocessing
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("en"))

DTM_test= DocumentTermMatrix(my_corpus_test,control = list(dictionary=Terms(DTM_train)))

DTM_test# has the same 660 terms

tfidf_test = weightTfIdf(DTM_test)
```

## Step 4: Summarizing the Term matrices by using PCA


```{r}
# PCA on the TF-IDF weights

tfidf_train_df <- as.data.frame(as.matrix(tfidf_train))
tfidf_test_df <- as.data.frame(as.matrix(tfidf_test))

pc_author = prcomp(tfidf_train_df)
pc_author_test <- predict(pc_author, newdata = tfidf_test_df)
pc_author_test <- as.data.frame(pc_author_test)

pve = summary(pc_author)$importance[3,]
plot(pve)
```

Since we cant see much of an elbow, we will cut at 75.

## Step 5: Exploring Classification Models

### Model 1: Boosting

```{r,cache=TRUE}
library(gbm)
#mode("X") = "numeric"
set.seed(12345)

n_cut = 75
X = pc_author$x[,1:n_cut]
y = sapply(strsplit(names(all_docs_train), "/"), "[", 3)
X_test = pc_author_test[,1:n_cut]
y_test = sapply(strsplit(names(all_docs_test), "/"), "[", 3)

TrainSet <- cbind(as.data.frame(X),y)
ValidSet <- cbind(as.data.frame(X_test),y_test)

boost.author <- gbm(y ~.,data=TrainSet, n.trees =100 ,shrinkage = 0.01,distribution = "multinomial",interaction.depth=4, cv.folds = 5)
summary(boost.author)

#== checking accuracy on trainset
pred=as.data.frame(predict(boost.author,newdata =TrainSet,n.trees=100,type="response"))
pred_val = sub("*\\.[0-9]+", "", colnames(pred)[apply(pred,1,which.max)])
mean(pred_val== y )

#== checking accuracy on test data
pred=as.data.frame(predict(boost.author,newdata =ValidSet,n.trees=100,type="response"))
pred_val = sub("*\\.[0-9]+", "", colnames(pred)[apply(pred,1,which.max)])
mean(pred_val== y )

```

Boosting classifies the training data well, but is unable to classify the test data.

### Model 2: Random Forest

```{r,cache=TRUE}
library(randomForest)
set.seed(12345)
n_cut = 150
X = pc_author$x[,1:n_cut]
y = sapply(strsplit(names(all_docs_train), "/"), "[", 3)
X_test = pc_author_test[,1:n_cut]
y_test = sapply(strsplit(names(all_docs_test), "/"), "[", 3)

TrainSet <- cbind(as.data.frame(X),y)
ValidSet <- cbind(as.data.frame(X_test),y_test)

rffit <- randomForest(y~.,TrainSet,ntree=200)
mean(predict(rffit,ValidSet)== y )

```

Random Forest does a better job than classification as compared to Boosting.


## Model 3: Naive Bayes

To avoid combinatorial explosion, we will have to build 1 classifier per author

```{r,cache=TRUE}
library(foreach)
library(doParallel)
set.seed(12345)


author_names = unique(sapply(strsplit(names(all_docs_train), "/"), "[", 3))
author_names_train = sapply(strsplit(names(all_docs_train), "/"), "[", 3)
author_names_test = sapply(strsplit(names(all_docs_test), "/"), "[", 3)
D = ncol(tfidf_train_df)
#initializing a matrix to store the output of 50 Naive Bayes models
NB_MODEL = matrix(0,2500,50)

#creating training and testing data
X_train = tfidf_train_df + 1/D
X_test  = tfidf_test_df + 1/D

for (i in 1:50){
  print("Running model for:")
  print(author_names[i])
  y_train = 0+{author_names_train == author_names[i]}
  y_test = 0+{author_names_test == author_names[i]}
  pvec_0 = colSums(X_train[y_train==0,])
  pvec_0 = pvec_0/sum(pvec_0)
  pvec_1 = colSums(X_train[y_train==1,])
  pvec_1 = pvec_1/sum(pvec_1)
  
  #Running the classifier on test data
  yhat_test = foreach(j = 1:2500,.combine='c') %dopar% {
  test_doc = X_test[j,]
  logp0 = sum(test_doc * log(pvec_0))
  logp1 = sum(test_doc * log(pvec_1))
  0 + {logp1 > logp0} #probabilty of that author
  }
  NB_MODEL[,i] = yhat_test

}

write.csv(NB_MODEL,"nb_model.csv")
```

# Question 3

```{r}
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
library(stringr)
set.seed(12345)

groceries <- read.transactions("groceries.txt", format = "basket", sep = ",",
                  cols = NULL, rm.duplicates = FALSE, 
                  quote = "\"'", skip = 0, 
                  encoding = "unknown")

groctrans = as(groceries, "transactions")
summary(groctrans)

# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 5
grocrules = apriori(groctrans, 
	parameter=list(support=.005, confidence=.2, maxlen=5))

inspect(grocrules)

plot(grocrules, measure = c("support", "lift"), shading = "confidence")

inspect(subset(grocrules, subset=lift > 5))

inspect(subset(grocrules, support > 0.035))
inspect(subset(grocrules, confidence > 0.7))


# support: Fraction of transactions that contain X and Y
# confidence: Measures how often items in Y appear in X

grocrules = apriori(groctrans, 
	parameter=list(support=.001, confidence=.6,target ="rules",maxlen=5,minlen=2))

arules::itemFrequencyPlot(groceries,topN=20,col=brewer.pal(8,'Pastel2'),main='Relative Item Frequency Plot',type="relative",ylab="Item Frequency (Relative))")
inspect(grocrules[1:20])

arulesViz::plotly_arules(grocrules)

plot(grocrules[1:20],method = "graph",control = list(type = "items"))

plot(grocrules[1:20], method = "paracoord",control = list(reorder = TRUE))

saveAsGraph(head(grocrules[1:200], n = 1000, by = "lift"), file = "grocrules.graphml")
```

Things we tried:

1. We didnt want whole milk. High confidence and low lift means 



